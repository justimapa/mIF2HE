{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import tables\n",
    "import openslide\n",
    "from misc.utils import extract_patch, draw_circle , is_nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 11000_13000, 18474\n",
      "Adding 11000_17000, 6738\n",
      "Adding 11000_21000, 15876\n",
      "Adding 11000_25000, 10351\n",
      "Adding 11000_29000, 7882\n",
      "Adding 11000_33000, 10534\n",
      "Adding 11000_37000, 11690\n",
      "Adding 11000_41000, 12766\n",
      "Adding 11000_45000, 7344\n",
      "Adding 11000_49000, 8077\n",
      "Adding 11000_53000, 3238\n",
      "[Errno 2] No such file or directory: './locations/3_MEL3_HE_cells_locations_NoOffset_1_4000pix_11000_57000.txt'\n",
      "[Errno 2] No such file or directory: './locations/3_MEL3_HE_cells_locations_NoOffset_1_4000pix_11000_61000.txt'\n",
      "Adding 11000_65000, 0\n",
      "Adding 11000_69000, 771\n",
      "Adding 11000_73000, 1693\n",
      "Adding 11000_77000, 2662\n",
      "Adding 11000_81000, 312\n",
      "Adding 15000_9000, 9890\n",
      "Adding 15000_13000, 6210\n",
      "Adding 15000_17000, 7795\n",
      "Adding 15000_21000, 6481\n",
      "Adding 15000_25000, 4031\n",
      "Adding 15000_29000, 3805\n",
      "Adding 15000_33000, 9224\n",
      "Adding 15000_37000, 14285\n",
      "Adding 15000_41000, 36\n",
      "Adding 15000_45000, 0\n",
      "Adding 15000_49000, 1891\n",
      "Adding 15000_53000, 4337\n",
      "Adding 15000_57000, 845\n",
      "Adding 15000_61000, 571\n",
      "Adding 15000_65000, 614\n",
      "Adding 15000_69000, 1445\n",
      "Adding 15000_73000, 28\n",
      "Adding 15000_77000, 1174\n",
      "Adding 15000_81000, 740\n",
      "Adding 19000_9000, 6260\n",
      "Adding 19000_13000, 6044\n",
      "Adding 19000_17000, 6149\n",
      "Adding 19000_21000, 3825\n",
      "Adding 19000_25000, 3124\n",
      "Adding 19000_29000, 3513\n",
      "Adding 19000_33000, 3980\n",
      "Adding 19000_37000, 9744\n",
      "[Errno 2] No such file or directory: './locations/3_MEL3_HE_cells_locations_NoOffset_1_4000pix_19000_41000.txt'\n",
      "Adding 19000_45000, 0\n",
      "Adding 19000_49000, 1932\n",
      "Adding 19000_53000, 4222\n",
      "Adding 19000_57000, 1099\n",
      "Adding 19000_61000, 3591\n",
      "Adding 19000_65000, 1966\n",
      "Adding 19000_69000, 3859\n",
      "Adding 19000_73000, 3058\n",
      "Adding 19000_77000, 3318\n",
      "Adding 19000_81000, 3267\n",
      "Adding 23000_9000, 8762\n",
      "Adding 23000_13000, 7235\n",
      "Adding 23000_17000, 5809\n",
      "Adding 23000_21000, 4051\n",
      "Adding 23000_25000, 2812\n",
      "Adding 23000_29000, 3277\n",
      "Adding 23000_33000, 4700\n",
      "Adding 23000_37000, 7836\n",
      "Adding 23000_41000, 158\n",
      "Adding 23000_45000, 1392\n",
      "Adding 23000_49000, 8835\n",
      "Adding 23000_53000, 4266\n",
      "Adding 23000_57000, 2764\n",
      "Adding 23000_61000, 3494\n",
      "Adding 23000_65000, 4834\n",
      "Adding 23000_69000, 4107\n",
      "Adding 23000_73000, 3240\n",
      "Adding 23000_77000, 2096\n",
      "Adding 23000_81000, 3527\n",
      "Adding 27000_9000, 15746\n",
      "Adding 27000_13000, 8302\n",
      "Adding 27000_17000, 5625\n",
      "Adding 27000_21000, 4709\n",
      "Adding 27000_25000, 2764\n",
      "Adding 27000_29000, 2939\n",
      "Adding 27000_33000, 1819\n",
      "Adding 27000_37000, 5722\n",
      "Adding 27000_41000, 347\n",
      "Adding 27000_45000, 1880\n",
      "Adding 27000_49000, 8182\n",
      "Adding 27000_53000, 4763\n",
      "Adding 27000_57000, 5339\n",
      "Adding 27000_61000, 4921\n",
      "Adding 27000_65000, 4828\n",
      "Adding 27000_69000, 5132\n",
      "Adding 27000_73000, 2859\n",
      "Adding 27000_77000, 2066\n",
      "Adding 27000_81000, 1168\n",
      "Adding 31000_9000, 14226\n",
      "Adding 31000_13000, 6634\n",
      "Adding 31000_17000, 5098\n",
      "Adding 31000_21000, 5374\n",
      "Adding 31000_25000, 2659\n",
      "Adding 31000_29000, 2055\n",
      "Adding 31000_33000, 2380\n",
      "Adding 31000_37000, 2471\n",
      "Adding 31000_41000, 148\n",
      "Adding 31000_45000, 1218\n",
      "Adding 31000_49000, 7087\n",
      "Adding 31000_53000, 5507\n",
      "Adding 31000_57000, 4362\n",
      "Adding 31000_61000, 2664\n",
      "Adding 31000_65000, 5216\n",
      "Adding 31000_69000, 2714\n",
      "Adding 31000_73000, 3712\n",
      "Adding 31000_77000, 922\n",
      "Adding 31000_81000, 130\n",
      "Adding 35000_9000, 10748\n",
      "Adding 35000_13000, 7794\n",
      "Adding 35000_17000, 1923\n",
      "Adding 35000_21000, 4796\n",
      "Adding 35000_25000, 3743\n",
      "Adding 35000_29000, 3130\n",
      "Adding 35000_33000, 2646\n",
      "Adding 35000_37000, 662\n",
      "Adding 35000_41000, 284\n",
      "Adding 35000_45000, 3493\n",
      "Adding 35000_49000, 10065\n",
      "Adding 35000_53000, 3388\n",
      "Adding 35000_57000, 1180\n",
      "Adding 35000_61000, 2252\n",
      "Adding 35000_65000, 2162\n",
      "Adding 35000_69000, 4930\n",
      "Adding 35000_73000, 3892\n",
      "Adding 35000_77000, 2147\n",
      "Adding 35000_81000, 823\n",
      "Adding 39000_9000, 5234\n",
      "Adding 39000_13000, 8373\n",
      "Adding 39000_17000, 7725\n",
      "Adding 39000_21000, 4436\n",
      "Adding 39000_25000, 3870\n",
      "Adding 39000_29000, 2243\n",
      "Adding 39000_33000, 887\n",
      "Adding 39000_37000, 440\n",
      "Adding 39000_41000, 6141\n",
      "Adding 39000_45000, 5761\n",
      "Adding 39000_49000, 4546\n",
      "Adding 39000_53000, 2320\n",
      "Adding 39000_57000, 539\n",
      "Adding 39000_61000, 923\n",
      "Adding 39000_65000, 3536\n",
      "Adding 39000_69000, 5600\n",
      "Adding 39000_73000, 4541\n",
      "Adding 39000_77000, 1502\n",
      "Adding 39000_81000, 2507\n",
      "Adding 43000_9000, 2086\n",
      "Adding 43000_13000, 8317\n",
      "Adding 43000_17000, 10165\n",
      "Adding 43000_21000, 6618\n",
      "Adding 43000_25000, 4410\n",
      "Adding 43000_29000, 748\n",
      "Adding 43000_33000, 109\n",
      "Adding 43000_37000, 4201\n",
      "Adding 43000_41000, 13980\n",
      "Adding 43000_45000, 13109\n",
      "Adding 43000_49000, 15827\n",
      "Adding 43000_53000, 12723\n",
      "Adding 43000_57000, 11491\n",
      "Adding 43000_61000, 9061\n",
      "Adding 43000_65000, 10444\n",
      "Adding 43000_69000, 4179\n",
      "Adding 43000_73000, 2432\n",
      "Adding 43000_77000, 2594\n",
      "Adding 43000_81000, 2777\n"
     ]
    }
   ],
   "source": [
    "# Load the H&E image\n",
    "HE_open = openslide.OpenSlide('Experiment/HE40X/MELANOMA_AA3_40X/Scan1/MELANOMA_AA3_40X_Scan1.qptiff')\n",
    "\n",
    "# Read H&E cells locations\n",
    "ROI_size = 4000\n",
    "for i in range(2,int(55840/ROI_size) - 2): \n",
    "    IF_origin = (270 + (i*ROI_size), 150 )\n",
    "    HE_origin = (3000 + (i*ROI_size), 1000 )\n",
    "    for j in range(2,int(93600/ROI_size) - 2):\n",
    "        IF_origin = (IF_origin[0], 150 + (j*ROI_size))\n",
    "        HE_origin = (HE_origin[0], 1000 + (j*ROI_size))\n",
    "        if i == 2 and j == 2:\n",
    "            Mel3_cells_positions = pd.read_csv(f'./locations/3_MEL3_HE_cells_locations_NoOffset_1_{ROI_size}pix_{HE_origin[0]}_{HE_origin[1]}.txt', header = None, names = ['x','y','type'], sep='\\t')\n",
    "            total = Mel3_cells_positions['x'].count()\n",
    "            continue\n",
    "        try : \n",
    "            df = pd.read_csv(f'./locations/3_MEL3_HE_cells_locations_NoOffset_1_{ROI_size}pix_{HE_origin[0]}_{HE_origin[1]}.txt', header = None, names = ['x','y','type'], sep='\\t')\n",
    "            print(f\"Adding {HE_origin[0]}_{HE_origin[1]}, {df['x'].count()}\")\n",
    "            total += df['x'].count()\n",
    "            Mel3_cells_positions = Mel3_cells_positions.merge(df, how='outer')\n",
    "        except OSError as e:\n",
    "            print(e)\n",
    "            pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x       797876\n",
       "y       797876\n",
       "type    797876\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mel3_cells_positions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CD11Cp</th>\n",
       "      <td>18141</td>\n",
       "      <td>18141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD20p</th>\n",
       "      <td>196629</td>\n",
       "      <td>196629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD3p</th>\n",
       "      <td>209678</td>\n",
       "      <td>209678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD56p</th>\n",
       "      <td>14348</td>\n",
       "      <td>14348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD68p</th>\n",
       "      <td>53896</td>\n",
       "      <td>53896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOX10p</th>\n",
       "      <td>305184</td>\n",
       "      <td>305184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x       y\n",
       "type                  \n",
       "CD11Cp   18141   18141\n",
       "CD20p   196629  196629\n",
       "CD3p    209678  209678\n",
       "CD56p    14348   14348\n",
       "CD68p    53896   53896\n",
       "SOX10p  305184  305184"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(f'Total cells count is {total}')\n",
    "# Count the number of cells per type \n",
    "Mel3_cells_positions.groupby(by='type').count()\n",
    "#Mel3_cells_positions.to_csv('MEL3_HE_cells_locations.txt', header=None, index = None, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phases = ['train','val']\n",
    "possible =['SOX10p','CD3p','CD20p','CD11Cp','CD68p','CD56p']\n",
    "# phenotypes that will be present in the database\n",
    "phenotypes = ['SOX10p','CD3p','CD20p']\n",
    "\n",
    "# defines the label associated to each class\n",
    "phenotype_mask = {'SOX10p':0, 'CD3p': 1,'CD20p' : 1}\n",
    "\n",
    "# number of classes\n",
    "nclasses = len(set(phenotype_mask.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "APC = ['CD11Cp' , 'CD68p']\n",
    "Immune = ['CD3p','CD20p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = Mel3_cells_positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APC\n",
      "Train {'CD11Cp': 0, 'CD68p': 0}\n",
      "Validation {'CD11Cp': 0, 'CD68p': 0}\n",
      "Test {'CD11Cp': 0, 'CD68p': 0}\n",
      "---------------------\n",
      "Immune cells\n",
      "Train {'CD3p': 125000, 'CD20p': 125000}\n",
      "Validation {'CD3p': 10000, 'CD20p': 10000}\n",
      "Test {'CD3p': 10000, 'CD20p': 10000}\n",
      "---------------------\n",
      "Tumour cells\n",
      "Train {'SOX10p': 250000}\n",
      "Validation {'SOX10p': 20000}\n",
      "Test {'SOX10p': 20000}\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data into training validation and test sets.\n",
    "\n",
    "size_APC_train = {'CD11Cp':0, 'CD68p': 0}\n",
    "size_APC_val = {'CD11Cp':0, 'CD68p': 0}\n",
    "size_APC_test = {'CD11Cp':0, 'CD68p': 0}\n",
    "\n",
    "size_immune_train = {'CD3p': 0,'CD20p' : 0}\n",
    "size_immune_val = {'CD3p': 0,'CD20p' : 0}\n",
    "size_immune_test = {'CD3p': 0,'CD20p' : 0}\n",
    "\n",
    "size_tumour_train =  {'SOX10p':0}\n",
    "size_tumour_val = {'SOX10p':0}\n",
    "size_tumour_test = {'SOX10p':0}\n",
    "\n",
    "\n",
    "for phenotype in APC:\n",
    "    size_APC_train [phenotype]=  0#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.8)\n",
    "    size_APC_val [phenotype]= 0#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.1)\n",
    "    size_APC_test [phenotype]= 0#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.1)\n",
    "    \n",
    "print(\"APC\")\n",
    "print(f\"Train {size_APC_train}\")\n",
    "print(f\"Validation {size_APC_val}\")\n",
    "print(f\"Test {size_APC_test}\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "for phenotype in Immune:\n",
    "    size_immune_train[phenotype]=  125000#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.8)\n",
    "    size_immune_val [phenotype]= 10000#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.1)\n",
    "    size_immune_test [phenotype]= 10000#int(Mel3_cells_positions[Mel3_cells_positions['type']==phenotype][\"x\"].count() * 0.1)\n",
    "    \n",
    "print(\"Immune cells\")\n",
    "print(f\"Train {size_immune_train}\")\n",
    "print(f\"Validation {size_immune_val}\")\n",
    "print(f\"Test {size_immune_test}\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "size_tumour_train [\"SOX10p\"]=  250000#int(Mel3_cells_positions[Mel3_cells_positions['type']==\"SOX10p\"][\"x\"].count() * 0.8)\n",
    "size_tumour_val [\"SOX10p\"]= 20000#int(Mel3_cells_positions[Mel3_cells_positions['type']==\"SOX10p\"][\"x\"].count() * 0.1)\n",
    "size_tumour_test [\"SOX10p\"]= 20000#int(Mel3_cells_positions[Mel3_cells_positions['type']==\"SOX10p\"][\"x\"].count() * 0.1)\n",
    "\n",
    "print(\"Tumour cells\")\n",
    "print(f\"Train {size_tumour_train}\")\n",
    "print(f\"Validation {size_tumour_val}\")\n",
    "print(f\"Test {size_tumour_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the patch\n",
    "patchsize = 32\n",
    "dataname = \"2_SOX10p_vs_CD3p_CD20p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train dataset... \n",
      "\n",
      "phenotype: SOX10p\n",
      "--> Tumour \n",
      "\n",
      "phenotype: CD3p\n",
      "--> Immune \n",
      "\n",
      "phenotype: CD20p\n",
      "--> Immune \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-82480e6df7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mtotal_immune\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msize_immune_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphenotype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mtotal_other\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msize_tumour_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SOX10p\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasssizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_immune\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mhdf5_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasssizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_other\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Extracting train dataset... \\n\")\n",
    "\n",
    "# size of the patch\n",
    "patchsize = 32\n",
    "\n",
    "\n",
    "\n",
    "# total size \n",
    "size = 0\n",
    "for phenotype in APC : \n",
    "    size += size_APC_train [phenotype]\n",
    "for phenotype in Immune : \n",
    "    size += size_immune_train [phenotype]\n",
    "size += size_tumour_train[\"SOX10p\"]\n",
    "\n",
    "# open a hdf5_file  in write mode\n",
    "hdf5_file = tables.open_file(f\"dataset/train_{size}_{patchsize}_{nclasses}classes_{dataname}.h5\", mode=\"w\", title=f\"Database test\")\n",
    "\n",
    "# define the shape of a patch, which is its size and the number of channels\n",
    "patch_shape = np.array((patchsize, patchsize, 3))\n",
    "\n",
    "filters = tables.Filters(complevel=6, complib='zlib')\n",
    "\n",
    "# earray for the patch\n",
    "hdf5_file.create_earray(hdf5_file.root, \"patch\", tables.UInt8Atom(), shape=np.append([0], patch_shape),\n",
    "                            chunkshape=np.append([1], patch_shape), filters=filters)\n",
    "\n",
    "# earray for the label\n",
    "hdf5_file.create_earray(hdf5_file.root, \"label\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the classsizes (important for neural network)\n",
    "hdf5_file.create_earray(hdf5_file.root, \"classsizes\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the image ID\n",
    "hdf5_file.create_earray(hdf5_file.root, \"imgID\", tables.UInt32Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "\n",
    "    \n",
    "for phenotype in phenotypes:\n",
    "        \n",
    "    print(f\"phenotype: {phenotype}\")\n",
    "    \n",
    "    if phenotype in Immune:\n",
    "        \n",
    "        print(\"--> Immune \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_immune_train[phenotype]).iterrows():\n",
    "            \n",
    "            # extract the patch\n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            # append all the information (patch, associated label, ID)\n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "\n",
    "            # remove this sample from the dataframe not to take it back after\n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "        \n",
    "    elif phenotype in APC: \n",
    "        \n",
    "        print(\"--> APC \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_APC_train[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "    else: \n",
    "        \n",
    "        print(\"--> Tumour \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_tumour_train[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "\n",
    "# add the classsizes for balancing the classes for training\n",
    "total_APC = 0\n",
    "for phenotype in APC : \n",
    "    total_APC += size_APC_train[phenotype]\n",
    "#To be improved\n",
    "total_other = 0\n",
    "total_immune = 0\n",
    "for phenotype in Immune : \n",
    "    total_immune += size_immune_train[phenotype]\n",
    "total_other += size_tumour_train[\"SOX10p\"]\n",
    "hdf5_file.root.classsizes.append([total_immune])\n",
    "hdf5_file.root.classsizes.append([total_other])\n",
    "# close the file \n",
    "hdf5_file.close()\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting val dataset... \n",
      "\n",
      "phenotype: SOX10p\n",
      "--> Tumour \n",
      "\n",
      "phenotype: CD3p\n",
      "--> Immune \n",
      "\n",
      "phenotype: CD20p\n",
      "--> Immune \n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting val dataset... \\n\")\n",
    "\n",
    "# total size \n",
    "size = 0\n",
    "for phenotype in APC : \n",
    "    size += size_APC_val [phenotype]\n",
    "for phenotype in Immune : \n",
    "    size += size_immune_val [phenotype]\n",
    "size += size_tumour_val[\"SOX10p\"]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "hdf5_file = tables.open_file(f\"dataset/val_{size}_{patchsize}_{nclasses}classes_{dataname}.h5\", mode=\"w\", title=f\"Database test\")\n",
    "    \n",
    "# define the shape of a patch, which is its size and the number of channels\n",
    "patch_shape = np.array((patchsize, patchsize, 3))\n",
    "\n",
    "filters = tables.Filters(complevel=6, complib='zlib')\n",
    "\n",
    "# earray for the patch\n",
    "hdf5_file.create_earray(hdf5_file.root, \"patch\", tables.UInt8Atom(), shape=np.append([0], patch_shape),\n",
    "                            chunkshape=np.append([1], patch_shape), filters=filters)\n",
    "\n",
    "# earray for the label\n",
    "hdf5_file.create_earray(hdf5_file.root, \"label\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the classsizes (important for neural network)\n",
    "hdf5_file.create_earray(hdf5_file.root, \"classsizes\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the image ID\n",
    "hdf5_file.create_earray(hdf5_file.root, \"imgID\", tables.UInt32Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "\n",
    "    \n",
    "for phenotype in phenotypes:\n",
    "        \n",
    "    print(f\"phenotype: {phenotype}\")\n",
    "    \n",
    "    if phenotype in Immune:\n",
    "        \n",
    "        print(\"--> Immune \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_immune_val[phenotype]).iterrows():\n",
    "            \n",
    "            # extract the patch\n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            # append all the information (patch, associated label, ID)\n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "\n",
    "            # remove this sample from the dataframe not to take it back after\n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "        \n",
    "    elif phenotype in APC: \n",
    "        \n",
    "        print(\"--> APC \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_APC_val[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "    else: \n",
    "        \n",
    "        print(\"--> Tumour \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_tumour_val[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "\n",
    "# add the classsizes for balancing the classes for training\n",
    "total_APC = 0\n",
    "for phenotype in APC : \n",
    "    total_APC += size_APC_val[phenotype]\n",
    "#To be improved\n",
    "total_other = 0\n",
    "total_immune = 0\n",
    "for phenotype in Immune : \n",
    "    total_immune += size_immune_val[phenotype]\n",
    "total_other += size_tumour_val[\"SOX10p\"]\n",
    "\n",
    "hdf5_file.root.classsizes.append([total_immune])\n",
    "hdf5_file.root.classsizes.append([total_other])\n",
    "# close the file \n",
    "hdf5_file.close()\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test dataset... \n",
      "\n",
      "phenotype: SOX10p\n",
      "--> Tumour \n",
      "\n",
      "phenotype: CD3p\n",
      "--> Immune \n",
      "\n",
      "phenotype: CD20p\n",
      "--> Immune \n",
      "\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting test dataset... \\n\")\n",
    "\n",
    "# total size \n",
    "size = 0\n",
    "for phenotype in APC : \n",
    "    size += size_APC_test [phenotype]\n",
    "for phenotype in Immune : \n",
    "    size += size_immune_test [phenotype]\n",
    "size += size_tumour_test[\"SOX10p\"]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "hdf5_file = tables.open_file(f\"dataset/test_{size}_{patchsize}_{nclasses}classes_{dataname}.h5\", mode=\"w\", title=f\"Database test\")\n",
    "    \n",
    "# define the shape of a patch, which is its size and the number of channels\n",
    "patch_shape = np.array((patchsize, patchsize, 3))\n",
    "\n",
    "filters = tables.Filters(complevel=6, complib='zlib')\n",
    "\n",
    "# earray for the patch\n",
    "hdf5_file.create_earray(hdf5_file.root, \"patch\", tables.UInt8Atom(), shape=np.append([0], patch_shape),\n",
    "                            chunkshape=np.append([1], patch_shape), filters=filters)\n",
    "\n",
    "# earray for the label\n",
    "hdf5_file.create_earray(hdf5_file.root, \"label\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the classsizes (important for neural network)\n",
    "hdf5_file.create_earray(hdf5_file.root, \"classsizes\", tables.UInt16Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "# earray for the image ID\n",
    "hdf5_file.create_earray(hdf5_file.root, \"imgID\", tables.UInt32Atom(), shape=[0], chunkshape=[1],\n",
    "                            filters=filters)\n",
    "\n",
    "\n",
    "    \n",
    "for phenotype in phenotypes:\n",
    "        \n",
    "    print(f\"phenotype: {phenotype}\")\n",
    "    \n",
    "    if phenotype in Immune:\n",
    "        \n",
    "        print(\"--> Immune \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_immune_test[phenotype]).iterrows():\n",
    "            \n",
    "            # extract the patch\n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            # append all the information (patch, associated label, ID)\n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "\n",
    "            # remove this sample from the dataframe not to take it back after\n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "        \n",
    "    elif phenotype in APC: \n",
    "        \n",
    "        print(\"--> APC \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_APC_test[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "    else: \n",
    "        \n",
    "        print(\"--> Tumour \\n\")\n",
    "        \n",
    "        for index, row in df_filtered[df_filtered['type']==phenotype].sample(size_tumour_test[phenotype]).iterrows():\n",
    "            \n",
    "            patch = extract_patch(HE_open, int(row['x']), int(row['y']), patchsize)\n",
    "            \n",
    "            hdf5_file.root.patch.append(patch[None, ::])\n",
    "            hdf5_file.root.label.append([phenotype_mask[phenotype]])\n",
    "            hdf5_file.root.imgID.append([index])\n",
    "            \n",
    "            df_filtered.drop(index = index, inplace = True)\n",
    "\n",
    "\n",
    "# add the classsizes for balancing the classes for training\n",
    "total_APC = 0\n",
    "for phenotype in APC : \n",
    "    total_APC += size_APC_test[phenotype]\n",
    "    \n",
    "#To be improved\n",
    "total_other = 0\n",
    "total_immune = 0\n",
    "for phenotype in Immune : \n",
    "    total_immune += size_immune_test[phenotype]\n",
    "total_other += size_tumour_test[\"SOX10p\"]\n",
    "\n",
    "hdf5_file.root.classsizes.append([total_immune])\n",
    "hdf5_file.root.classsizes.append([total_other])\n",
    "# close the file \n",
    "hdf5_file.close()\n",
    "print(\"Completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
